{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing IPython cluster clients and printing their ids to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "from ipyparallel import Client\n",
    "\n",
    "clients = Client(cluster_id='mpi')\n",
    "clients.block = True  # use synchronous computations\n",
    "print(clients.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing mpi4py and numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel execution on engine(s): all\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "from mpi4py import MPI\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing Convolution, you don't need to modify this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel execution on engine(s): all\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "def convolve_func(main,kernel,KERNEL_DIM,DIMx,DIMy,upper_pad,lower_pad):\n",
    "\tnum_pads = int((KERNEL_DIM - 1) / 2)\n",
    "\tconv = np.zeros(main.shape,dtype=int)\n",
    "\tmain = np.concatenate((upper_pad,main,lower_pad))\n",
    "\tfor i in range(DIMy):\n",
    "\t\tfor j in range(DIMx):\n",
    "\t\t\tfor k in range(KERNEL_DIM):\n",
    "\t\t\t\tfor l in range(KERNEL_DIM):\n",
    "\t\t\t\t\tif j+l <= DIMx+1 and i+k>=num_pads and i+k<=DIMy:\n",
    "\t\t\t\t\t\tconv[j*DIMy+i] += main[(j+l)*DIMy+i-num_pads+k]#*kernel[k][l]\n",
    "\treturn conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 points: \n",
    "Load MPI communicator, get the total number of processes and rank of the process                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel execution on engine(s): all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] Hello from process 0 out of 3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:2] Hello from process 2 out of 3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] Hello from process 1 out of 3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "#and also print total number of processes and rank from each process\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD     \n",
    "rank = comm.Get_rank()    \n",
    "size = comm.Get_size()    \n",
    "\n",
    "print(f\"Hello from process {rank} out of {size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 points: \n",
    "Load or initialize data array and kernel array only in process 0(rank 0)                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel execution on engine(s): 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] [rank 0] img=(9, 4), kernel=(3, 3), DIMy=4, DIMx=9, KERNEL_DIM=3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px --targets 0\n",
    "DIMx = 0\n",
    "DIMy = 0\n",
    "KERNEL_DIM = 0\n",
    "img = None\n",
    "kernel = None\n",
    "#Add a condition such that these intializations below should happen in only process 0\n",
    "img = np.array([[3, 9, 5, 9],[1, 7, 4, 3],[2, 1, 6, 5],[3, 9, 5, 9],[1, 7, 4, 3],[2, 1, 6, 5],[3, 9, 5, 9],[1, 7, 4, 3],[2, 1, 6, 5]])\n",
    "kernel = np.array([[0, 1, 0],[0, 0, 0],[0, -1, 0]])\n",
    "DIMx = img.shape[0]\n",
    "DIMy = img.shape[1]\n",
    "KERNEL_DIM = int(kernel.shape[0])\n",
    "\n",
    "print(f\"[rank {rank}] img={None if img is None else img.shape}, \"\n",
    "      f\"kernel={None if kernel is None else kernel.shape}, \"\n",
    "      f\"DIMy={DIMy}, DIMx={DIMx}, KERNEL_DIM={KERNEL_DIM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 points: \n",
    "Broadcast data and kernel array sizes from process 0 to  all other processes                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel execution on engine(s): all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] [rank 0] received sizes -> DIMx=9, DIMy=4, KERNEL_DIM=3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] [rank 1] received sizes -> DIMx=9, DIMy=4, KERNEL_DIM=3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:2] [rank 2] received sizes -> DIMx=9, DIMy=4, KERNEL_DIM=3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "#broadcast data and kernel array sizes (think why we are broadcasting sizes)\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "DIMx = comm.bcast(DIMx if rank == 0 else None, root=0)\n",
    "DIMy = comm.bcast(DIMy if rank == 0 else None, root=0)\n",
    "KERNEL_DIM = comm.bcast(KERNEL_DIM if rank == 0 else None, root=0)\n",
    "\n",
    "print(f\"[rank {rank}] received sizes -> DIMx={DIMx}, DIMy={DIMy}, KERNEL_DIM={KERNEL_DIM}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize empty kernel array for all  processes except rank = 0, why we are not initialzing kernel array for rank 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:Kernel array is not initialized for rank 0 because process 0 already holds the real kernel data. Other processes only need an empty array to receive the kernel when it is broadcasted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel execution on engine(s): all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[stdout:2] [rank 2] kernel init -> zeros (3, 3)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] [rank 0] kernel init -> has data\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] [rank 1] kernel init -> zeros (3, 3)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "#initialize empty kernel array except for process 0(rank=0)\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank != 0:\n",
    "    kernel = np.zeros((KERNEL_DIM, KERNEL_DIM), dtype=int)\n",
    "\n",
    "print(f\"[rank {rank}] kernel init ->\",\n",
    "      \"has data\" if rank==0 else f\"zeros {kernel.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 points: \n",
    "Broadcast Kernel array from rank 0 to all other processes.                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel execution on engine(s): all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] [rank 0] kernel shape = (3, 3)  first_row = [0, 1, 0]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:2] [rank 2] kernel shape = (3, 3)  first_row = [0, 1, 0]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] [rank 1] kernel shape = (3, 3)  first_row = [0, 1, 0]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "#broadcast kernel array from rank 0 to all other processes\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "kernel = comm.bcast(kernel if rank == 0 else None, root=0)\n",
    "\n",
    "print(f\"[rank {rank}] kernel shape = {getattr(kernel, 'shape', None)}  first_row = {kernel[0].tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25 points: \n",
    "Split the rows in data array equally and scatter them from process 0 to all other process. To split them \n",
    "equally, number of rows in the data array must be a integral multiple of number of processes. MPI has ways \n",
    "to send unequal chunks of data between processses. But for here you can do with equal number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel execution on engine(s): all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] [rank 0] got block shape=(3, 4) (global rows 0:3) first_row=[3, 9, 5, 9]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] [rank 1] got block shape=(3, 4) (global rows 3:6) first_row=[3, 9, 5, 9]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:2] [rank 2] got block shape=(3, 4) (global rows 6:9) first_row=[3, 9, 5, 9]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "#split and send data array to corresponding processses (you need to initialize a buffer to receive data from \n",
    "#process 0, similar to the random initializing done for kernel array)\n",
    "\n",
    "\n",
    "#Here does we initialize buffer for process 0 also, if so why?(Hint: because of the function we are using to send \n",
    "#and receieve data)\n",
    "rows_per_proc = DIMx // size\n",
    "assert DIMx % size == 0, f\"DIMx({DIMx}) must be a multiple of size({size})\"\n",
    "\n",
    "if rank == 0:\n",
    "    chunks = [img[p*rows_per_proc:(p+1)*rows_per_proc, :] for p in range(size)]\n",
    "else:\n",
    "    chunks = None\n",
    "\n",
    "local_img = comm.scatter(chunks, root=0)\n",
    "\n",
    "gstart = rank * rows_per_proc\n",
    "gend   = (rank + 1) * rows_per_proc\n",
    "print(f\"[rank {rank}] got block shape={local_img.shape} \"\n",
    "      f\"(global rows {gstart}:{gend}) first_row={local_img[0].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25 points: \n",
    "For convolution of kernel array and data array, you have to pass the kernel padding rows from one\n",
    "process to another. please see objective for more details. Send and Recieve rows from one process \n",
    "to other. Careful with the data size and tags you are sending and receiving should match otherwise\n",
    "commincator will wait for them indefintely.                                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel execution on engine(s): all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] [rank 1] upper_pad.sum()=14  lower_pad.sum()=26  local_img shape=(3, 4)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:2] [rank 2] upper_pad.sum()=14  lower_pad.sum()=0  local_img shape=(3, 4)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] [rank 0] upper_pad.sum()=0  lower_pad.sum()=26  local_img shape=(3, 4)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "#send padding rows from one process to other (carefully observe which process to send data to which process and\n",
    "# which process receives the data)\n",
    "top_n = rank - 1 if rank > 0       else MPI.PROC_NULL\n",
    "bot_n = rank + 1 if rank < size-1  else MPI.PROC_NULL\n",
    "\n",
    "send_to_top = np.ascontiguousarray(local_img[0, :])\n",
    "send_to_bot = np.ascontiguousarray(local_img[-1, :])\n",
    "\n",
    "upper_pad = np.zeros((1, DIMy), dtype=int) if top_n == MPI.PROC_NULL else np.empty((1, DIMy), dtype=int)\n",
    "lower_pad = np.zeros((1, DIMy), dtype=int) if bot_n == MPI.PROC_NULL else np.empty((1, DIMy), dtype=int)\n",
    "\n",
    "comm.Sendrecv(sendbuf=send_to_top, dest=top_n, sendtag=11,\n",
    "              recvbuf=upper_pad[0, :], source=top_n, recvtag=22)\n",
    "\n",
    "comm.Sendrecv(sendbuf=send_to_bot, dest=bot_n, sendtag=22,\n",
    "              recvbuf=lower_pad[0, :], source=bot_n, recvtag=11)\n",
    "\n",
    "print(f\"[rank {rank}] upper_pad.sum()={int(upper_pad.sum())}  \"\n",
    "      f\"lower_pad.sum()={int(lower_pad.sum())}  \"\n",
    "      f\"local_img shape={local_img.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why we are loading data into process 0 and broadcasting input data to all other processes? are there any other methods to load data into all processes (not for evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:The data is first loaded in process 0 so it can send the data to all other processes. This makes it easier to manage and keeps the data the same for everyone. Another way is to let each process read its own part of the data at the same time, but that needs special support like MPI-IO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 points: \n",
    "Perform Convolution operation by calling convolve_func() provided for each of the process with \n",
    "corresponding rows as arguments.                                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel execution on engine(s): all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] [rank 1] local_conv shape = (12,)  first_row = 23\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] [rank 0] local_conv shape = (12,)  first_row = 20\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:2] [rank 2] local_conv shape = (12,)  first_row = 23\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "#convolution function arguments\n",
    "#main - data array (flattened array), only the part of the data array that is processed for each process\n",
    "#kernel - kernel array\n",
    "#DIMy - ColumnSize\n",
    "#DIMx - RowSize\n",
    "#upper_pad = upper padding row\n",
    "#lower_pad = lower padding row\n",
    "from mpi4py import MPI\n",
    "rank = MPI.COMM_WORLD.Get_rank()\n",
    "\n",
    "DIMx_local = local_img.shape[0]\n",
    "assert kernel.shape == (KERNEL_DIM, KERNEL_DIM), f\"rank {rank} kernel shape wrong: {kernel.shape}\"\n",
    "assert local_img.shape[1] == DIMy,               f\"rank {rank} DIMy mismatch: {local_img.shape}\"\n",
    "\n",
    "main_1d  = local_img.reshape(-1)          # or .ravel()\n",
    "up_1d    = upper_pad.reshape(-1)\n",
    "low_1d   = lower_pad.reshape(-1)\n",
    "\n",
    "local_conv = convolve_func(\n",
    "    main_1d,\n",
    "    kernel,\n",
    "    KERNEL_DIM,\n",
    "    DIMx_local,\n",
    "    DIMy,\n",
    "    up_1d,\n",
    "    low_1d\n",
    ")\n",
    "\n",
    "print(f\"[rank {rank}] local_conv shape = {local_conv.shape}  first_row = {local_conv[0].tolist() if hasattr(local_conv[0],'tolist') else local_conv[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 points: \n",
    "Gather the computed convolutional matrix rows to process 0.                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel execution on engine(s): all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] [rank 0] gathered length = 36, expected = 36\n",
       "[rank 0] head 8 values: [20, 29, 37, 21, 23, 38, 49, 32]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "#To receive data from all processes, process 0 should have a buffer\n",
    "sendbuf = local_conv.astype(int)\n",
    "\n",
    "recvbuf = None\n",
    "if rank == 0:\n",
    "    recvbuf = np.empty(DIMx_local * DIMy * size, dtype=int)\n",
    "comm.Gather(sendbuf, recvbuf, root=0)\n",
    "if rank == 0:\n",
    "    print(f\"[rank 0] gathered length = {recvbuf.size}, expected = {DIMx_local*DIMy*size}\")\n",
    "    print(f\"[rank 0] head 8 values:\", recvbuf[:8].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the flattened array to match input dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel execution on engine(s): all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] [rank 0] full_conv shape = (9, 4)\n",
       "[rank 0] first row:  [20, 29, 37, 21]\n",
       "[rank 0] middle row: [23, 38, 49, 32]\n",
       "[rank 0] last row:   [11, 21, 26, 18]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "#Reshape the collected array to the input image dimensions\n",
    "if rank == 0:\n",
    "    DIMx_full = DIMx_local * size\n",
    "    full_conv = recvbuf.reshape(DIMx_full, DIMy)\n",
    "\n",
    "    print(f\"[rank 0] full_conv shape = {full_conv.shape}\")\n",
    "    print(f\"[rank 0] first row:  {full_conv[0].tolist()}\")\n",
    "    print(f\"[rank 0] middle row: {full_conv[DIMx_local].tolist()}\")   \n",
    "    print(f\"[rank 0] last row:   {full_conv[-1].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 points: \n",
    "Test to check sequential convolution and MPI based parallel convolution outputs                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel execution on engine(s): all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] True\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "\n",
    "\n",
    "if rank == 0:\n",
    "    #main_grid is the actual input input image array that is flattened\n",
    "    #convolution function arguments\n",
    "    #main_grid - data array (flattened array)\n",
    "    #kernel - kernel array\n",
    "    #DIMy - ColumnSize\n",
    "    #Dimx - RowSize\n",
    "    #upper_pad = upper padding row\n",
    "    #lower_pad = lower padding row\n",
    "    up0  = np.zeros((1, DIMy), dtype=int)\n",
    "    low0 = np.zeros((1, DIMy), dtype=int)\n",
    "    #rename the below arguments according to your variable names\n",
    "    \n",
    "    #Entire convolution in a single process\n",
    "    conv1 = convolve_func(\n",
    "        img.reshape(-1),\n",
    "        kernel,\n",
    "        KERNEL_DIM,\n",
    "        DIMx,\n",
    "        DIMy,\n",
    "        up0.reshape(-1),\n",
    "        low0.reshape(-1)\n",
    "    )\n",
    "    conv1 = conv1.reshape(DIMx, DIMy)\n",
    "    #recvbuf is the convolution computed by parallel processes and gathered in process 0, \n",
    "    #if you named it different, modify that name below\n",
    "    \n",
    "    #Checking with parallel convolution output\n",
    "    print(np.array_equal(conv1.ravel(), recvbuf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
